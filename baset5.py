# -*- coding: utf-8 -*-
"""BaseT5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ePrni3ravVKoSQSKKgxQ21KzBKGdyMo9

Preprocessing Script
"""

import requests
import json
import pandas as pd
import openai  # Ensure you've installed the openai Python package
import re

def remove_code_blocks(text):
    """
    Remove all substrings contained between <pre><code> and </pre></code> tags.
    """
    cleaned_text = re.sub(r'<pre><code>.*?</pre></code>', '', text, flags=re.DOTALL)
    return cleaned_text

def fetch_question_bodies(api_key, num_questions=5000, sort_criteria='activity', start_page=55):
    url = "https://api.stackexchange.com/2.3/questions"
    params = {
        'pagesize': 100,
        'order': 'desc',
        'sort': sort_criteria,
        'site': 'stackoverflow',
        'key': api_key,
        'page': start_page,
        'filter': 'withbody'
    }
    questions = []
    while len(questions) < num_questions:
        response = requests.get(url, params=params)
        data = response.json()
        if 'items' not in data:
            print("No more questions or error occurred:", data.get('error_message', 'No additional error information.'))
            break

        for question in data['items']:
            questions.append({
                'id': question['question_id'],
                'body': question['body']
            })
            if len(questions) >= num_questions:
                break

        if not data.get('has_more', False) or len(questions) >= num_questions:
            break

        params['page'] += 1

    return questions

def fetch_first_answer(question_id, api_key):
    url = f"https://api.stackexchange.com/2.3/questions/{question_id}/answers"
    params = {
        'site': 'stackoverflow',
        'key': api_key,
        'order': 'asc',
        'sort': 'creation',
        'pagesize': 1,
        'filter': 'withbody'
    }
    response = requests.get(url, params=params)
    data = response.json()
    if 'items' in data and data['items']:
        return data['items'][0]['body']
    return "No answer found"

def improve_question_syntax(question):
#add open ai key here
    openai.api_key = ""

    # Constructing the complete prompt as a single user message
    complete_prompt = "You are a helpful assistant who improves the question writing of Stack Overflow questions and returns the result in JSON with a field name as 'rev'. Don't answer; just return the improved question. Question: " + question

    # Use the OpenAI API to get a response
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        response_format={ "type": "json_object" },
        messages=[
            {"role": "user", "content": complete_prompt}
        ]
    )

    # Extracting the response assuming the response contains a JSON string
    response_content = response['choices'][0]['message']['content']
    print(response_content)  # Debug: print the JSON response

    # Convert the JSON string to a Python dictionary
    improved_question_data = json.loads(response_content)

    # Print and return the 'rev' key containing the revised question
    print(improved_question_data["rev"])
    return improved_question_data["rev"]

def main(api_key, sort_criteria='activity', start_page=1):
    questions = fetch_question_bodies(api_key, 5000, sort_criteria, start_page)
    results = []
    for q in questions:
        cleaned_body = remove_code_blocks(q['body'])
        answer = fetch_first_answer(q['id'], api_key)
        improved_question = improve_question_syntax(cleaned_body)
        results.append({
            'Original Question': remove_code_blocks(q['body']),
            'Improved Question': improved_question,
            'First Answer': answer
        })

    if results:
        df = pd.DataFrame(results)
        df.to_csv('stackoverflow_question_enhancements.csv', index=False)
        print("Results saved to CSV.")
    else:
        print("No question bodies found.")

if __name__ == "__main__":
#add api key for stack below
    api_key = ‘’

    main(api_key)

"""Library Installation"""

!pip install transformers
!pip install sentencepiece

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP Data

import pandas as pd
from sklearn.model_selection import train_test_split

import torch

"""Preprocessed Data from GDrive"""

data = pd.read_csv("cleaned_stackoverflow_questions.csv")

data.head(25)

data = data[["Original Question", "Improved Question"]]
data.head(25)

# Split the dataset into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)

# Convert the data to lists
train_original_questions = train_data["Original Question"].tolist()
train_improved_questions = train_data["Improved Question"].tolist()

val_original_questions = val_data["Original Question"].tolist()
val_improved_questions = val_data["Improved Question"].tolist()

val_data.head()

from transformers import T5Tokenizer, T5ForConditionalGeneration

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

"""Tokenising Data for Finetuning"""

tokenizer = T5Tokenizer.from_pretrained('avirathtibrewala/results')
model = T5ForConditionalGeneration.from_pretrained('avirathtibrewala/results').to(device)

from tqdm.auto import tqdm

"""Prediction Generation Code"""

def generate_predictions(questions, batch_size=10):
    model.eval()  # Set the model to evaluation mode
    predictions = []
    # Process questions in batches
    for i in tqdm(range(0, len(questions), batch_size), desc="Processing batches"):
        batch_questions = questions[i:i + batch_size]
        inputs = tokenizer(batch_questions, return_tensors="pt", padding=True, truncation=True, max_length=512)

        # Move tensors to the correct device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'],
                                     max_length=512, num_beams=5, early_stopping=True)

        for output in outputs:
            corrected_question = tokenizer.decode(output, skip_special_tokens=True)
            predictions.append(corrected_question)
    return predictions

# Example usage (make sure val_data['Original Question'] is properly formatted as a list)
val_data['Model Prediction'] = generate_predictions(val_data['Original Question'].tolist(), batch_size=10)

val_data.head(15)

val_data.to_csv('validation_data_with_t5ftpredictions.csv', index=False)

"""Initially tried BLEU scores"""

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import nltk

# Download the tokenizer resource if not already present
nltk.download('punkt')

def calculate_scores(predictions, references):
    # Initialize Rouge
    # rouge = Rouge()
    # rouge_scores = rouge.get_scores(predictions, references, avg=True)

    # Compute BLEU scores
    bleu_scores = []
    for pred, ref in zip(predictions, references):
        # Tokenize the sentences
        ref_tokens = nltk.word_tokenize(ref)
        pred_tokens = nltk.word_tokenize(pred)

        # Calculate BLEU score, using smoothing in case of zero matches
        score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1)
        bleu_scores.append(score)

    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)

    # Print the scores
    # print("ROUGE-1 F1 Score:", rouge_scores["rouge-1"]["f"])
    # print("ROUGE-2 F1 Score:", rouge_scores["rouge-2"]["f"])
    # print("ROUGE-L F1 Score:", rouge_scores["rouge-l"]["f"])
    print("Average BLEU Score:", avg_bleu_score)

# Example usage
# calculate_scores(val_data['Model Prediction'], val_data['Revised Question'])

val_data['Model Prediction']

"""Finetuning Model"""

import pandas as pd
import torch
import warnings
from sklearn.model_selection import train_test_split
from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from datasets import Dataset
from tqdm.auto import tqdm

# Ignore tokenizer overflow warning
warnings.filterwarnings("ignore", message=".*overflowing tokens are not returned.*")

# Load the CSV file
data = pd.read_csv("cleaned_stackoverflow_questions.csv")

# Select the required columns
data = data[["Original Question", "Improved Question"]]

# Split the dataset into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)

# Convert DataFrame to Hugging Face dataset format
train_dataset = Dataset.from_pandas(train_data)
val_dataset = Dataset.from_pandas(val_data)

# Load the pre-trained T5 model and tokenizer
model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Move model to GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Tokenize the datasets
def tokenize_data(example):
    inputs = tokenizer(example['Original Question'], padding="max_length", truncation=True, max_length=512, return_tensors="pt")
    outputs = tokenizer(example['Improved Question'], padding="max_length", truncation=True, max_length=512, return_tensors="pt")

    return {
        "input_ids": inputs.input_ids.squeeze(),
        "attention_mask": inputs.attention_mask.squeeze(),
        "labels": outputs.input_ids.squeeze()  # Use squeeze to remove the batch dimension
    }

train_dataset = train_dataset.map(tokenize_data, batched=False)
val_dataset = val_dataset.map(tokenize_data, batched=False)

# Set up the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./finetuned_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=5,
    predict_with_generate=True,
    fp16=True,
    load_best_model_at_end=True
)

# Create the Trainer with tqdm progress bar integrated in Hugging Face
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=collator
)

# Fine-tune the model with tqdm
trainer.train()

# Generate predictions on the validation set and evaluate
outputs = trainer.predict(val_dataset)
decoded_preds = tokenizer.batch_decode(outputs.predictions, skip_special_tokens=True)

# Uncomment to evaluate ROUGE scores if you've installed the rouge-score package
# rouge = load_metric("rouge")
# scores = rouge.compute(predictions=decoded_preds, references=val_dataset["Improved Question"])
# print("ROUGE-1 F1 Score:", scores["rouge1"].mid.fmeasure)
# print("ROUGE-2 F1 Score:", scores["rouge2"].mid.fmeasure)
# print("ROUGE-L F1 Score:", scores["rougeL"].mid.fmeasure)

print(val_data.dtypes)

val_data['Original Question'] = val_data['Original Question'].astype(str)
val_data['Revised Question'] = val_data['Improved Question'].astype(str)

print(val_data.dtypes)

for col in ['Original Question', 'Improved Question']:
    try:
        val_data[col] = val_data[col].apply(lambda x: str(x))
    except Exception as e:
        print(f"Error converting {col} to string: {e}")

calculate_scores(val_data['Model Prediction'], val_data['Improved Question'])

val_data.head(15)

# Assuming val_data is your DataFrame with a new column 'Model Prediction' containing the generated predictions
val_data.to_csv('validation_data_with_predictions.csv', index=False)

!pip install rouge nltk

from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import nltk

# Download the tokenizer resource if not already present
nltk.download('punkt')

def calculate_scores(predictions, references):
    # Initialize Rouge
    # rouge = Rouge()
    # rouge_scores = rouge.get_scores(predictions, references, avg=True)

    # Compute BLEU scores
    bleu_scores = []
    for pred, ref in zip(predictions, references):
        # Tokenize the sentences
        ref_tokens = nltk.word_tokenize(ref)
        pred_tokens = nltk.word_tokenize(pred)

        # Calculate BLEU score, using smoothing in case of zero matches
        score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1)
        bleu_scores.append(score)

    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)

    # Print the scores
    # print("ROUGE-1 F1 Score:", rouge_scores["rouge-1"]["f"])
    # print("ROUGE-2 F1 Score:", rouge_scores["rouge-2"]["f"])
    # print("ROUGE-L F1 Score:", rouge_scores["rouge-l"]["f"])
    print("Average BLEU Score:", avg_bleu_score)

# Example usage
# calculate_scores(val_data['Model Prediction'], val_data['Revised Question'])

val_data['Model Prediction']

print(val_data.dtypes)

val_data['Original Question'] = val_data['Original Question'].astype(str)
val_data['Revised Question'] = val_data['Improved Question'].astype(str)

print(val_data.dtypes)

for col in ['Original Question', 'Improved Question']:
    try:
        val_data[col] = val_data[col].apply(lambda x: str(x))
    except Exception as e:
        print(f"Error converting {col} to string: {e}")

calculate_scores(val_data['Model Prediction'], val_data['Improved Question'])

!pip install datasets

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP Data

!pip install accelerate -U

!pip install transformers[torch] -U

print("Accelerate version:", accelerate.__version__)

!pip install accelerate transformers[torch] -U

import accelerate
print("Accelerate version:", accelerate.__version__)

import transformers
print("Transformers version:", transformers.__version__)

!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP Data

import pandas as pd
import torch
import warnings
from sklearn.model_selection import train_test_split
from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from datasets import Dataset
from tqdm.auto import tqdm

# Ignore tokenizer overflow warning
warnings.filterwarnings("ignore", message=".*overflowing tokens are not returned.*")

# Load the CSV file
data = pd.read_csv("cleaned_stackoverflow_questions.csv")

# Select the required columns
data = data[["Original Question", "Improved Question"]]

# Split the dataset into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)

# Convert DataFrame to Hugging Face dataset format
train_dataset = Dataset.from_pandas(train_data)
val_dataset = Dataset.from_pandas(val_data)

# Load the pre-trained T5 model and tokenizer
model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Move model to GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Tokenize the datasets
def tokenize_data(example):
    inputs = tokenizer(example['Original Question'], padding="max_length", truncation=True, max_length=512, return_tensors="pt")
    outputs = tokenizer(example['Improved Question'], padding="max_length", truncation=True, max_length=512, return_tensors="pt")

    return {
        "input_ids": inputs.input_ids.squeeze(),
        "attention_mask": inputs.attention_mask.squeeze(),
        "labels": outputs.input_ids.squeeze()  # Use squeeze to remove the batch dimension
    }

train_dataset = train_dataset.map(tokenize_data, batched=False)
val_dataset = val_dataset.map(tokenize_data, batched=False)

# Set up the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./finetuned_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=5,
    predict_with_generate=True,
    fp16=True,
    load_best_model_at_end=True
)

# Create the Trainer with tqdm progress bar integrated in Hugging Face
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=collator
)

# Fine-tune the model with tqdm
trainer.train()

# Generate predictions on the validation set and evaluate
outputs = trainer.predict(val_dataset)
decoded_preds = tokenizer.batch_decode(outputs.predictions, skip_special_tokens=True)

# Uncomment to evaluate ROUGE scores if you've installed the rouge-score package
# rouge = load_metric("rouge")
# scores = rouge.compute(predictions=decoded_preds, references=val_dataset["Improved Question"])
# print("ROUGE-1 F1 Score:", scores["rouge1"].mid.fmeasure)
# print("ROUGE-2 F1 Score:", scores["rouge2"].mid.fmeasure)
# print("ROUGE-L F1 Score:", scores["rougeL"].mid.fmeasure)

decoded_preds

def calculate_scores(predictions, references):
    # Initialize Rouge
    # rouge = Rouge()
    # rouge_scores = rouge.get_scores(predictions, references, avg=True)

    # Compute BLEU scores
    bleu_scores = []
    for pred, ref in zip(predictions, references):
        # Tokenize the sentences
        ref_tokens = nltk.word_tokenize(ref)
        pred_tokens = nltk.word_tokenize(pred)

        # Calculate BLEU score, using smoothing in case of zero matches
        score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1)
        bleu_scores.append(score)

    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)

    # Print the scores
    # print("ROUGE-1 F1 Score:", rouge_scores["rouge-1"]["f"])
    # print("ROUGE-2 F1 Score:", rouge_scores["rouge-2"]["f"])
    # print("ROUGE-L F1 Score:", rouge_scores["rouge-l"]["f"])
    print("Average BLEU Score:", avg_bleu_score)

# Example usage
# calculate_scores(val_data['Model Prediction'], val_data['Revised Question'])

calculate_scores(decoded_preds, val_data['Revised Question'])

!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116

!pip install accelerate -U
!pip install transformers[torch]

!pip install datasets transformers==4.17 rouge_score nltk torch

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP Data

!pip install accelerate -U
!pip install transformers[torch]

import transformers
from datasets import load_dataset, load_metric

from datasets import load_dataset

# Load your dataset
cleaned_datasets = load_dataset("csv", data_files="./cleaned_stackoverflow_questions.csv")

# Shuffle the dataset (specify a seed for reproducibility if needed)
cleaned_datasets = cleaned_datasets["train"].shuffle(seed=42)

# Split the shuffled dataset into training and testing sets
cleaned_datasets = cleaned_datasets.train_test_split(test_size=0.15)

# Access the train and test sets
train_data = cleaned_datasets['train']
test_data = cleaned_datasets['test']

import nltk
nltk.download('punkt')
import string
from transformers import AutoTokenizer

model_checkpoint = "jeremyvictor/flan-t5-base-clang8-e1-b16"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)



# # Load model directly
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# tokenizer = AutoTokenizer.from_pretrained("gotutiyan/gec-t5-base-clang8")
# model = AutoModelForSeq2SeqLM.from_pretrained("gotutiyan/gec-t5-base-clang8")

train_data

prefix = "grammar correction: "
max_input_length = 512
max_target_length = 512

def preprocess_data(examples):
    inputs = [prefix + question for question in examples["Original Question"]]
    targets = [q if isinstance(q, str) and q else "default string" for q in examples["Improved Question"]]


    print("Type of targets:", type(targets))
    print("First few targets:", targets[:5])
    print("Type of inputs:", type(inputs))
    print("First few inputs:", inputs[:5])


    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')

    # Setup the tokenizer for targets
    # with tokenizer.as_target_tokenizer():

    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding='max_length')

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply preprocessing to the train and test datasets
train_dataset = train_data.map(preprocess_data, batched=True)
test_dataset = test_data.map(preprocess_data, batched=True)



import accelerate
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer


# Load the pre-trained T5 model
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# Define the data collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True,    pad_to_multiple_of=8,

    return_tensors="pt"
)

# Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./resultsCLANG",
        fp16=True,

    gradient_accumulation_steps=4,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=6,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=3,
    predict_with_generate=True,
)

# Create the trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Fine-tune the model
trainer.train()

from huggingface_hub import notebook_login

notebook_login()

# trainer.push_to_hub(repo_name="nlpproj")
# adityapalliyil0/t5clangfinetuned

from huggingface_hub import HfApi, HfFolder

api = HfApi()
repo

# Commented out IPython magic to ensure Python compatibility.
# %cd ./resultsCLANG

!ls



from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP Data

import pandas as pd
from sklearn.model_selection import train_test_split

import torch

data = pd.read_csv("validation_data_with_t5ftpredictions.csv")

data.head(100)

!pip install rouge
from rouge import Rouge
rouge = Rouge()
scores = rouge.get_scores(data['Original Question'], data['Model Prediction'])

# Print the different ROUGE scores
print("ROUGE-1 F1 Score:", scores[0]['rouge-1']['f'])
print("ROUGE-2 F1 Score:", scores[0]['rouge-2']['f'])
print("ROUGE-L F1 Score:", scores[0]['rouge-l']['f'])



!pip install transformers
!pip install sentencepiece

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP Data

import pandas as pd
from sklearn.model_selection import train_test_split

import torch

data = pd.read_csv("cleaned_stackoverflow_questions.csv")

data = data[["Original Question", "Improved Question"]]
data.head(25)

# Split the dataset into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)

# Convert the data to lists
train_original_questions = train_data["Original Question"].tolist()
train_improved_questions = train_data["Improved Question"].tolist()

val_original_questions = val_data["Original Question"].tolist()
val_improved_questions = val_data["Improved Question"].tolist()

from transformers import T5Tokenizer, T5ForConditionalGeneration

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

tokenizer = T5Tokenizer.from_pretrained('jeremyvictor/flan-t5-base-clang8-e1-b16')
model = T5ForConditionalGeneration.from_pretrained('jeremyvictor/flan-t5-base-clang8-e1-b16').to(device)

from tqdm.auto import tqdm

def generate_predictions(questions, batch_size=10):
    model.eval()  # Set the model to evaluation mode
    predictions = []
    # Process questions in batches
    for i in tqdm(range(0, len(questions), batch_size), desc="Processing batches"):
        batch_questions = questions[i:i + batch_size]
        inputs = tokenizer(batch_questions, return_tensors="pt", padding=True, truncation=True, max_length=512)

        # Move tensors to the correct device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'],
                                     max_length=512, num_beams=5, early_stopping=True)

        for output in outputs:
            corrected_question = tokenizer.decode(output, skip_special_tokens=True)
            predictions.append(corrected_question)
    return predictions

# Example usage (make sure val_data['Original Question'] is properly formatted as a list)
val_data['Model Prediction'] = generate_predictions(val_data['Original Question'].tolist(), batch_size=10)



# Assuming val_data is your DataFrame with a new column 'Model Prediction' containing the generated predictions
val_data.to_csv('validation_data_clang.csv', index=False)

data = pd.read_csv("validation_data_clang.csv")

data.head()

rouge = Rouge()
scores = rouge.get_scores(data['Original Question'], data['Model Prediction'])

# Print the different ROUGE scores
print("ROUGE-1 F1 Score:", scores[0]['rouge-1']['f'])
print("ROUGE-2 F1 Score:", scores[0]['rouge-2']['f'])
print("ROUGE-L F1 Score:", scores[0]['rouge-l']['f'])

